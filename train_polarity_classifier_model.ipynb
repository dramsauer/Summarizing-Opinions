{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers for Sentiment Analysis\n",
    "\n",
    "+++ This notebook is adapted from transformers notebook in https://github.com/bentrevett/pytorch-sentiment-analysis +++\n",
    "____________\n",
    "\n",
    "In this notebook we will be using the transformer model, first introduced in [this](https://arxiv.org/abs/1706.03762) paper. Specifically, we will be using the BERT (Bidirectional Encoder Representations from Transformers) model from [this](https://arxiv.org/abs/1810.04805) paper. \n",
    "\n",
    "Transformer models are considerably larger than anything else covered in these tutorials. As such we are going to use the [transformers library](https://github.com/huggingface/transformers) to get pre-trained transformers and use them as our embedding layers. We will freeze (not train) the transformer and only train the remainder of the model which learns from the representations produced by the transformer. In this case we will be using a multi-layer bi-directional GRU, however any model can learn from these representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data\n",
    "\n",
    "First, as always, let's set the random seeds for deterministic results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (1.6.0)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (3.1.0)\n",
      "Requirement already satisfied: torchtext in /opt/conda/lib/python3.7/site-packages (0.7.0)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch) (0.18.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch) (1.18.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers) (20.1)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.1.91)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied: tokenizers==0.8.1.rc2 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.8.1rc2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2020.7.14)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (2.4.6)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (1.14.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.25.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch transformers torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformer has already been trained with a specific vocabulary, which means we need to train with the exact same vocabulary and also tokenize our data in the same way that the transformer did when it was initially trained.\n",
    "\n",
    "Luckily, the transformers library has tokenizers for each of the transformer models provided. In this case we are using the BERT model which ignores casing (i.e. will lower case every word). We get this by loading the pre-trained `bert-base-uncased` tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tokenizer` has a `vocab` attribute which contains the actual vocabulary we will be using. We can check how many tokens are in it by checking its length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the tokenizer is as simple as calling `tokenizer.tokenize` on a string. This will tokenize and lower case the data in a way that is consistent with the pre-trained transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', 'how', 'are', 'you', '?']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize('Hello WORLD how ARE yoU?')\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can numericalize tokens using our vocabulary using `tokenizer.convert_tokens_to_ids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7592, 2088, 2129, 2024, 2017, 1029]\n"
     ]
    }
   ],
   "source": [
    "indexes = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformer was also trained with special tokens to mark the beginning and end of the sentence, detailed [here](https://huggingface.co/transformers/model_doc/bert.html#transformers.BertModel). As well as a standard padding and unknown token. We can also get these from the tokenizer.\n",
    "\n",
    "**Note**: the tokenizer does have a beginning of sequence and end of sequence attributes (`bos_token` and `eos_token`) but these are not set and should not be used for this transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] [SEP] [PAD] [UNK]\n"
     ]
    }
   ],
   "source": [
    "init_token = tokenizer.cls_token\n",
    "eos_token = tokenizer.sep_token\n",
    "pad_token = tokenizer.pad_token\n",
    "unk_token = tokenizer.unk_token\n",
    "\n",
    "print(init_token, eos_token, pad_token, unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the indexes of the special tokens by converting them using the vocabulary..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 102 0 100\n"
     ]
    }
   ],
   "source": [
    "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
    "eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
    "\n",
    "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...or by explicitly getting them from the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 102 0 100\n"
     ]
    }
   ],
   "source": [
    "init_token_idx = tokenizer.cls_token_id\n",
    "eos_token_idx = tokenizer.sep_token_id\n",
    "pad_token_idx = tokenizer.pad_token_id\n",
    "unk_token_idx = tokenizer.unk_token_id\n",
    "\n",
    "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing we need to handle is that the model was trained on sequences with a defined maximum length - it does not know how to handle sequences longer than it has been trained on. We can get the maximum length of these input sizes by checking the `max_model_input_sizes` for the version of the transformer we want to use. In this case, it is 512 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
    "\n",
    "print(max_input_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we have used the `spaCy` tokenizer to tokenize our examples. However we now need to define a function that we will pass to our `TEXT` field that will handle all the tokenization for us. It will also cut down the number of tokens to a maximum length. Note that our maximum length is 2 less than the actual maximum length. This is because we need to append two tokens to each sequence, one to the start and one to the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_cut(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence) \n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define our fields. The transformer expects the batch dimension to be first, so we set `batch_first = True`. As we already have the vocabulary for our text, provided by the transformer we set `use_vocab = False` to tell torchtext that we'll be handling the vocabulary side of things. We pass our `tokenize_and_cut` function as the tokenizer. The `preprocessing` argument is a function that takes in the example after it has been tokenized, this is where we will convert the tokens to their indexes. Finally, we define the special tokens - making note that we are defining them to be their index value and not their string value, i.e. `100` instead of `[UNK]` This is because the sequences will already be converted into indexes.\n",
    "\n",
    "We define the label field as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: LabelField class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from torchtext import data\n",
    "\n",
    "TEXT = data.Field(batch_first = True,\n",
    "                  use_vocab = False,\n",
    "                  tokenize = tokenize_and_cut,\n",
    "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "                  init_token = init_token_idx,\n",
    "                  eos_token = eos_token_idx,\n",
    "                  pad_token = pad_token_idx,\n",
    "                  unk_token = unk_token_idx)\n",
    "\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data and create the validation splits as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from torchtext import datasets\n",
    "\n",
    "### train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "mpqa_sentiment_dataset = data.TabularDataset(\n",
    "    path='data/mpqa/all-bcbfeed8.tsv', format='tsv',\n",
    "    fields=(\n",
    "        ('text', TEXT), \n",
    "        ('label', LABEL)\n",
    "    ),\n",
    "    skip_header=True\n",
    ")\n",
    "train_data, test_data = mpqa_sentiment_dataset.split()\n",
    "\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check an example and ensure that the text has already been numericalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': [2009, 4150, 1010, 1999, 1037, 6881, 2126, 1010, 2625, 21668], 'label': 'neg'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `convert_ids_to_tokens` to transform these indexes back into readable tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'becomes', ',', 'in', 'a', 'weird', 'way', ',', 'less', 'monstrous']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(vars(train_data.examples[3])['text'])\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 5197\n",
      "Number of validation examples: 2227\n",
      "Number of testing examples: 3182\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_data)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data)}\")\n",
    "print(f\"Number of testing examples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we've handled the vocabulary for the text, we still need to build the vocabulary for the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'neg': 0, 'pos': 1})\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we create the iterators. Ideally we want to use the largest batch size that we can as I've found this gives the best results for transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Since we do not use a standard dataset but a custom one, this\n",
    "# might cause problems such as a CUDNN_STATUS_EXECUTION_FAILED \n",
    "# error. Changes a made here: *\n",
    "# https://github.com/pytorch/text/issues/474\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    device = device,\n",
    "    sort=False   # *\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model\n",
    "\n",
    "Next, we'll load the pre-trained model, making sure to load the same model as we did for the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define our actual model. \n",
    "\n",
    "Instead of using an embedding layer to get embeddings for our text, we'll be using the pre-trained transformer model. These embeddings will then be fed into a GRU to produce a prediction for the sentiment of the input sentence. We get the embedding dimension size (called the `hidden_size`) from the transformer via its config attribute. The rest of the initialization is standard.\n",
    "\n",
    "Within the forward pass, we wrap the transformer in a `no_grad` to ensure no gradients are calculated over this part of the model. The transformer actually returns the embeddings for the whole sequence as well as a *pooled* output. The [documentation](https://huggingface.co/transformers/model_doc/bert.html#transformers.BertModel) states that the pooled output is \"usually not a good summary of the semantic content of the input, youâ€™re often better with averaging or pooling the sequence of hidden-states for the whole input sequence\", hence we will not be using it. The rest of the forward pass is the standard implementation of a recurrent model, where we take the hidden state over the final time-step, and pass it through a linear layer to get our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_gru_sentiment import BERTGRUSentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create an instance of our model using standard hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.25\n",
    "\n",
    "model = BERTGRUSentiment(bert,\n",
    "                         HIDDEN_DIM,\n",
    "                         OUTPUT_DIM,\n",
    "                         N_LAYERS,\n",
    "                         BIDIRECTIONAL,\n",
    "                         DROPOUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check how many parameters the model has. The standard models from the tutorial have under 5M, but this one has 112M! Luckily, 110M of these parameters are from the transformer and we will not be training those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 112,241,409 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to freeze paramers (not train them) we need to set their `requires_grad` attribute to `False`. To do this, we simply loop through all of the `named_parameters` in our model and if they're a part of the `bert` transformer model, we set `requires_grad = False`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():                \n",
    "    if name.startswith('bert'):\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that our model has under 3M trainable parameters, making it almost comparable to the `FastText` model. However, the text still has to propagate through the transformer which causes training to take considerably longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,759,169 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can double check the names of the trainable parameters, ensuring they make sense. As we can see, they are all the parameters of the GRU (`rnn`) and the linear layer (`out`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn.weight_ih_l0\n",
      "rnn.weight_hh_l0\n",
      "rnn.bias_ih_l0\n",
      "rnn.bias_hh_l0\n",
      "rnn.weight_ih_l0_reverse\n",
      "rnn.weight_hh_l0_reverse\n",
      "rnn.bias_ih_l0_reverse\n",
      "rnn.bias_hh_l0_reverse\n",
      "rnn.weight_ih_l1\n",
      "rnn.weight_hh_l1\n",
      "rnn.bias_ih_l1\n",
      "rnn.bias_hh_l1\n",
      "rnn.weight_ih_l1_reverse\n",
      "rnn.weight_hh_l1_reverse\n",
      "rnn.bias_ih_l1_reverse\n",
      "rnn.bias_hh_l1_reverse\n",
      "out.weight\n",
      "out.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():                \n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "As is standard, we define our optimizer and criterion (loss function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Place the model and criterion onto the GPU (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define functions for: calculating accuracy, performing a training epoch, performing an evaluation epoch and calculating how long a training/evaluation epoch takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll train our model. This takes considerably longer than any of the previous models due to the size of the transformer. Even though we are not training any of the transformer's parameters we still need to pass the data through the model which takes a considerable amount of time on a standard GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT_FILENAME = 'output/opinion_polarity/BERTGRUSentiment-model-MPQA_v2_100.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_data = {\n",
    "    'Epoch': [], \n",
    "    'Train Loss': [],\n",
    "    'Train Acc': [],\n",
    "    'Val. Loss': [],\n",
    "    'Val. Acc': [],\n",
    "    'Epoch Time elapsed': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.070 | Train Acc: 96.78%\n",
      "\t Val. Loss: 0.544 |  Val. Acc: 88.19%\n",
      "Epoch: 02 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.073 | Train Acc: 97.04%\n",
      "\t Val. Loss: 0.503 |  Val. Acc: 87.54%\n",
      "Epoch: 03 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.068 | Train Acc: 97.09%\n",
      "\t Val. Loss: 0.547 |  Val. Acc: 87.48%\n",
      "Epoch: 04 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.066 | Train Acc: 97.32%\n",
      "\t Val. Loss: 0.549 |  Val. Acc: 87.37%\n",
      "Epoch: 05 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.073 | Train Acc: 96.90%\n",
      "\t Val. Loss: 0.521 |  Val. Acc: 87.52%\n",
      "Epoch: 06 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.075 | Train Acc: 97.12%\n",
      "\t Val. Loss: 0.509 |  Val. Acc: 87.39%\n",
      "Epoch: 07 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.072 | Train Acc: 97.02%\n",
      "\t Val. Loss: 0.515 |  Val. Acc: 87.69%\n",
      "Epoch: 08 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.064 | Train Acc: 97.54%\n",
      "\t Val. Loss: 0.560 |  Val. Acc: 87.11%\n",
      "Epoch: 09 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.062 | Train Acc: 97.32%\n",
      "\t Val. Loss: 0.583 |  Val. Acc: 87.58%\n",
      "Epoch: 10 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.064 | Train Acc: 97.32%\n",
      "\t Val. Loss: 0.593 |  Val. Acc: 88.19%\n",
      "Epoch: 11 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.065 | Train Acc: 97.14%\n",
      "\t Val. Loss: 0.553 |  Val. Acc: 86.43%\n",
      "Epoch: 12 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.069 | Train Acc: 97.35%\n",
      "\t Val. Loss: 0.584 |  Val. Acc: 87.50%\n",
      "Epoch: 13 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.063 | Train Acc: 97.30%\n",
      "\t Val. Loss: 0.601 |  Val. Acc: 86.89%\n",
      "Epoch: 14 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.068 | Train Acc: 97.19%\n",
      "\t Val. Loss: 0.593 |  Val. Acc: 86.89%\n",
      "Epoch: 15 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.060 | Train Acc: 97.38%\n",
      "\t Val. Loss: 0.647 |  Val. Acc: 87.32%\n",
      "Epoch: 16 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.064 | Train Acc: 97.23%\n",
      "\t Val. Loss: 0.548 |  Val. Acc: 87.04%\n",
      "Epoch: 17 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.065 | Train Acc: 97.57%\n",
      "\t Val. Loss: 0.591 |  Val. Acc: 87.45%\n",
      "Epoch: 18 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.074 | Train Acc: 96.73%\n",
      "\t Val. Loss: 0.563 |  Val. Acc: 87.54%\n",
      "Epoch: 19 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.059 | Train Acc: 97.43%\n",
      "\t Val. Loss: 0.551 |  Val. Acc: 87.32%\n",
      "Epoch: 20 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.069 | Train Acc: 97.35%\n",
      "\t Val. Loss: 0.531 |  Val. Acc: 86.41%\n",
      "Epoch: 21 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.068 | Train Acc: 97.05%\n",
      "\t Val. Loss: 0.536 |  Val. Acc: 87.97%\n",
      "Epoch: 22 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.063 | Train Acc: 97.43%\n",
      "\t Val. Loss: 0.589 |  Val. Acc: 86.93%\n",
      "Epoch: 23 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.062 | Train Acc: 97.39%\n",
      "\t Val. Loss: 0.556 |  Val. Acc: 87.35%\n",
      "Epoch: 24 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.065 | Train Acc: 97.26%\n",
      "\t Val. Loss: 0.535 |  Val. Acc: 87.28%\n",
      "Epoch: 25 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.061 | Train Acc: 97.23%\n",
      "\t Val. Loss: 0.617 |  Val. Acc: 86.87%\n",
      "Epoch: 26 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.064 | Train Acc: 97.35%\n",
      "\t Val. Loss: 0.590 |  Val. Acc: 88.43%\n",
      "Epoch: 27 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.058 | Train Acc: 97.62%\n",
      "\t Val. Loss: 0.660 |  Val. Acc: 87.06%\n",
      "Epoch: 28 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.052 | Train Acc: 97.72%\n",
      "\t Val. Loss: 0.604 |  Val. Acc: 87.41%\n",
      "Epoch: 29 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.056 | Train Acc: 97.71%\n",
      "\t Val. Loss: 0.635 |  Val. Acc: 87.43%\n",
      "Epoch: 30 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.061 | Train Acc: 97.46%\n",
      "\t Val. Loss: 0.655 |  Val. Acc: 87.24%\n",
      "Epoch: 31 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.063 | Train Acc: 97.17%\n",
      "\t Val. Loss: 0.662 |  Val. Acc: 87.24%\n",
      "Epoch: 32 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.057 | Train Acc: 97.59%\n",
      "\t Val. Loss: 0.602 |  Val. Acc: 87.37%\n",
      "Epoch: 33 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.065 | Train Acc: 97.45%\n",
      "\t Val. Loss: 0.650 |  Val. Acc: 87.08%\n",
      "Epoch: 34 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.061 | Train Acc: 97.59%\n",
      "\t Val. Loss: 0.660 |  Val. Acc: 85.61%\n",
      "Epoch: 35 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.066 | Train Acc: 97.32%\n",
      "\t Val. Loss: 0.624 |  Val. Acc: 87.80%\n",
      "Epoch: 36 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.065 | Train Acc: 97.28%\n",
      "\t Val. Loss: 0.576 |  Val. Acc: 87.39%\n",
      "Epoch: 37 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.053 | Train Acc: 97.97%\n",
      "\t Val. Loss: 0.625 |  Val. Acc: 87.06%\n",
      "Epoch: 38 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.063 | Train Acc: 97.50%\n",
      "\t Val. Loss: 0.572 |  Val. Acc: 86.65%\n",
      "Epoch: 39 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.051 | Train Acc: 97.72%\n",
      "\t Val. Loss: 0.611 |  Val. Acc: 87.00%\n",
      "Epoch: 40 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.056 | Train Acc: 97.54%\n",
      "\t Val. Loss: 0.630 |  Val. Acc: 87.35%\n",
      "Epoch: 41 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.071 | Train Acc: 97.08%\n",
      "\t Val. Loss: 0.608 |  Val. Acc: 86.65%\n",
      "Epoch: 42 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.055 | Train Acc: 97.63%\n",
      "\t Val. Loss: 0.592 |  Val. Acc: 87.08%\n",
      "Epoch: 43 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.061 | Train Acc: 97.62%\n",
      "\t Val. Loss: 0.597 |  Val. Acc: 87.09%\n",
      "Epoch: 44 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.070 | Train Acc: 97.27%\n",
      "\t Val. Loss: 0.565 |  Val. Acc: 87.45%\n",
      "Epoch: 45 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.071 | Train Acc: 97.01%\n",
      "\t Val. Loss: 0.585 |  Val. Acc: 87.43%\n",
      "Epoch: 46 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.066 | Train Acc: 97.15%\n",
      "\t Val. Loss: 0.571 |  Val. Acc: 87.43%\n",
      "Epoch: 47 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.051 | Train Acc: 97.76%\n",
      "\t Val. Loss: 0.615 |  Val. Acc: 87.00%\n",
      "Epoch: 48 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.048 | Train Acc: 97.94%\n",
      "\t Val. Loss: 0.658 |  Val. Acc: 87.02%\n",
      "Epoch: 49 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.055 | Train Acc: 97.71%\n",
      "\t Val. Loss: 0.668 |  Val. Acc: 86.61%\n",
      "Epoch: 50 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.060 | Train Acc: 97.54%\n",
      "\t Val. Loss: 0.646 |  Val. Acc: 87.71%\n",
      "Epoch: 51 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.057 | Train Acc: 97.67%\n",
      "\t Val. Loss: 0.610 |  Val. Acc: 87.30%\n",
      "Epoch: 52 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.054 | Train Acc: 97.78%\n",
      "\t Val. Loss: 0.629 |  Val. Acc: 88.00%\n",
      "Epoch: 53 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.059 | Train Acc: 97.55%\n",
      "\t Val. Loss: 0.663 |  Val. Acc: 87.15%\n",
      "Epoch: 54 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.062 | Train Acc: 97.54%\n",
      "\t Val. Loss: 0.663 |  Val. Acc: 85.91%\n",
      "Epoch: 55 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.070 | Train Acc: 97.11%\n",
      "\t Val. Loss: 0.668 |  Val. Acc: 86.19%\n",
      "Epoch: 56 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.053 | Train Acc: 97.62%\n",
      "\t Val. Loss: 0.627 |  Val. Acc: 88.08%\n",
      "Epoch: 57 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.057 | Train Acc: 97.47%\n",
      "\t Val. Loss: 0.643 |  Val. Acc: 87.78%\n",
      "Epoch: 58 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.059 | Train Acc: 97.61%\n",
      "\t Val. Loss: 0.665 |  Val. Acc: 87.19%\n",
      "Epoch: 59 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.063 | Train Acc: 97.12%\n",
      "\t Val. Loss: 0.652 |  Val. Acc: 87.04%\n",
      "Epoch: 60 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.064 | Train Acc: 97.48%\n",
      "\t Val. Loss: 0.632 |  Val. Acc: 86.26%\n",
      "Epoch: 61 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.060 | Train Acc: 97.39%\n",
      "\t Val. Loss: 0.578 |  Val. Acc: 88.26%\n",
      "Epoch: 62 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.052 | Train Acc: 97.73%\n",
      "\t Val. Loss: 0.666 |  Val. Acc: 87.69%\n",
      "Epoch: 63 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.062 | Train Acc: 97.30%\n",
      "\t Val. Loss: 0.632 |  Val. Acc: 87.76%\n",
      "Epoch: 64 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.053 | Train Acc: 97.69%\n",
      "\t Val. Loss: 0.628 |  Val. Acc: 87.74%\n",
      "Epoch: 65 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.056 | Train Acc: 97.80%\n",
      "\t Val. Loss: 0.630 |  Val. Acc: 86.45%\n",
      "Epoch: 66 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.052 | Train Acc: 97.57%\n",
      "\t Val. Loss: 0.630 |  Val. Acc: 87.56%\n",
      "Epoch: 67 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.054 | Train Acc: 97.66%\n",
      "\t Val. Loss: 0.593 |  Val. Acc: 87.52%\n",
      "Epoch: 68 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.054 | Train Acc: 97.76%\n",
      "\t Val. Loss: 0.583 |  Val. Acc: 86.69%\n",
      "Epoch: 69 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.050 | Train Acc: 97.92%\n",
      "\t Val. Loss: 0.654 |  Val. Acc: 86.15%\n",
      "Epoch: 70 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.058 | Train Acc: 97.74%\n",
      "\t Val. Loss: 0.609 |  Val. Acc: 86.82%\n",
      "Epoch: 71 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.055 | Train Acc: 97.67%\n",
      "\t Val. Loss: 0.611 |  Val. Acc: 87.30%\n",
      "Epoch: 72 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.053 | Train Acc: 97.82%\n",
      "\t Val. Loss: 0.621 |  Val. Acc: 87.93%\n",
      "Epoch: 73 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.054 | Train Acc: 97.78%\n",
      "\t Val. Loss: 0.574 |  Val. Acc: 87.58%\n",
      "Epoch: 74 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.052 | Train Acc: 97.80%\n",
      "\t Val. Loss: 0.639 |  Val. Acc: 87.67%\n",
      "Epoch: 75 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.058 | Train Acc: 97.46%\n",
      "\t Val. Loss: 0.647 |  Val. Acc: 87.30%\n",
      "Epoch: 76 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.045 | Train Acc: 97.97%\n",
      "\t Val. Loss: 0.709 |  Val. Acc: 87.19%\n",
      "Epoch: 77 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.051 | Train Acc: 97.82%\n",
      "\t Val. Loss: 0.622 |  Val. Acc: 86.89%\n",
      "Epoch: 78 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.050 | Train Acc: 97.73%\n",
      "\t Val. Loss: 0.687 |  Val. Acc: 86.78%\n",
      "Epoch: 79 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.054 | Train Acc: 97.51%\n",
      "\t Val. Loss: 0.705 |  Val. Acc: 87.30%\n",
      "Epoch: 80 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.053 | Train Acc: 97.82%\n",
      "\t Val. Loss: 0.680 |  Val. Acc: 87.65%\n",
      "Epoch: 81 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.063 | Train Acc: 97.41%\n",
      "\t Val. Loss: 0.651 |  Val. Acc: 87.39%\n",
      "Epoch: 82 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.051 | Train Acc: 97.90%\n",
      "\t Val. Loss: 0.626 |  Val. Acc: 87.56%\n",
      "Epoch: 83 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.056 | Train Acc: 97.83%\n",
      "\t Val. Loss: 0.602 |  Val. Acc: 87.91%\n",
      "Epoch: 84 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.053 | Train Acc: 97.80%\n",
      "\t Val. Loss: 0.635 |  Val. Acc: 87.61%\n",
      "Epoch: 85 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.054 | Train Acc: 97.78%\n",
      "\t Val. Loss: 0.632 |  Val. Acc: 87.63%\n",
      "Epoch: 86 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.056 | Train Acc: 97.40%\n",
      "\t Val. Loss: 0.682 |  Val. Acc: 86.43%\n",
      "Epoch: 87 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.052 | Train Acc: 97.77%\n",
      "\t Val. Loss: 0.667 |  Val. Acc: 87.37%\n",
      "Epoch: 88 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.050 | Train Acc: 98.06%\n",
      "\t Val. Loss: 0.757 |  Val. Acc: 86.69%\n",
      "Epoch: 89 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.051 | Train Acc: 97.92%\n",
      "\t Val. Loss: 0.697 |  Val. Acc: 87.13%\n",
      "Epoch: 90 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.061 | Train Acc: 97.33%\n",
      "\t Val. Loss: 0.669 |  Val. Acc: 87.19%\n",
      "Epoch: 91 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.049 | Train Acc: 97.98%\n",
      "\t Val. Loss: 0.667 |  Val. Acc: 87.61%\n",
      "Epoch: 92 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.053 | Train Acc: 97.85%\n",
      "\t Val. Loss: 0.644 |  Val. Acc: 87.93%\n",
      "Epoch: 93 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.050 | Train Acc: 97.64%\n",
      "\t Val. Loss: 0.680 |  Val. Acc: 87.93%\n",
      "Epoch: 94 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.051 | Train Acc: 97.87%\n",
      "\t Val. Loss: 0.632 |  Val. Acc: 87.02%\n",
      "Epoch: 95 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.053 | Train Acc: 97.85%\n",
      "\t Val. Loss: 0.680 |  Val. Acc: 87.87%\n",
      "Epoch: 96 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.051 | Train Acc: 97.83%\n",
      "\t Val. Loss: 0.690 |  Val. Acc: 87.41%\n",
      "Epoch: 97 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.053 | Train Acc: 97.78%\n",
      "\t Val. Loss: 0.661 |  Val. Acc: 87.28%\n",
      "Epoch: 98 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.046 | Train Acc: 98.11%\n",
      "\t Val. Loss: 0.685 |  Val. Acc: 87.30%\n",
      "Epoch: 99 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.052 | Train Acc: 97.84%\n",
      "\t Val. Loss: 0.634 |  Val. Acc: 87.17%\n",
      "Epoch: 100 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.051 | Train Acc: 98.04%\n",
      "\t Val. Loss: 0.661 |  Val. Acc: 87.19%\n",
      "Epoch: 101 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.050 | Train Acc: 97.88%\n",
      "\t Val. Loss: 0.697 |  Val. Acc: 87.52%\n",
      "Epoch: 102 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.056 | Train Acc: 97.60%\n",
      "\t Val. Loss: 0.653 |  Val. Acc: 87.63%\n",
      "Epoch: 103 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.044 | Train Acc: 98.15%\n",
      "\t Val. Loss: 0.701 |  Val. Acc: 87.02%\n",
      "Epoch: 104 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.052 | Train Acc: 97.62%\n",
      "\t Val. Loss: 0.641 |  Val. Acc: 87.52%\n",
      "Epoch: 105 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.054 | Train Acc: 97.78%\n",
      "\t Val. Loss: 0.645 |  Val. Acc: 87.32%\n",
      "Epoch: 106 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.054 | Train Acc: 97.74%\n",
      "\t Val. Loss: 0.617 |  Val. Acc: 88.04%\n",
      "Epoch: 107 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.054 | Train Acc: 97.69%\n",
      "\t Val. Loss: 0.662 |  Val. Acc: 87.78%\n",
      "Epoch: 108 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.044 | Train Acc: 97.98%\n",
      "\t Val. Loss: 0.712 |  Val. Acc: 87.87%\n",
      "Epoch: 109 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.065 | Train Acc: 97.35%\n",
      "\t Val. Loss: 0.665 |  Val. Acc: 86.72%\n",
      "Epoch: 110 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.052 | Train Acc: 97.74%\n",
      "\t Val. Loss: 0.640 |  Val. Acc: 87.48%\n",
      "Epoch: 111 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.047 | Train Acc: 98.13%\n",
      "\t Val. Loss: 0.665 |  Val. Acc: 87.67%\n",
      "Epoch: 112 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.043 | Train Acc: 98.16%\n",
      "\t Val. Loss: 0.646 |  Val. Acc: 88.00%\n",
      "Epoch: 113 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.056 | Train Acc: 97.75%\n",
      "\t Val. Loss: 0.611 |  Val. Acc: 88.08%\n",
      "Epoch: 114 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.047 | Train Acc: 97.90%\n",
      "\t Val. Loss: 0.646 |  Val. Acc: 87.95%\n",
      "Epoch: 115 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.055 | Train Acc: 97.63%\n",
      "\t Val. Loss: 0.684 |  Val. Acc: 86.45%\n",
      "Epoch: 116 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.049 | Train Acc: 98.18%\n",
      "\t Val. Loss: 0.642 |  Val. Acc: 87.24%\n",
      "Epoch: 117 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.052 | Train Acc: 97.82%\n",
      "\t Val. Loss: 0.599 |  Val. Acc: 87.82%\n",
      "Epoch: 118 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.055 | Train Acc: 97.75%\n",
      "\t Val. Loss: 0.606 |  Val. Acc: 87.78%\n",
      "Epoch: 119 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.049 | Train Acc: 97.93%\n",
      "\t Val. Loss: 0.658 |  Val. Acc: 87.58%\n",
      "Epoch: 120 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.048 | Train Acc: 97.73%\n",
      "\t Val. Loss: 0.699 |  Val. Acc: 87.50%\n",
      "Epoch: 121 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.049 | Train Acc: 97.74%\n",
      "\t Val. Loss: 0.648 |  Val. Acc: 86.00%\n",
      "Epoch: 122 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.050 | Train Acc: 97.94%\n",
      "\t Val. Loss: 0.655 |  Val. Acc: 86.74%\n",
      "Epoch: 123 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.043 | Train Acc: 98.04%\n",
      "\t Val. Loss: 0.655 |  Val. Acc: 87.30%\n",
      "Epoch: 124 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.047 | Train Acc: 97.96%\n",
      "\t Val. Loss: 0.656 |  Val. Acc: 86.80%\n",
      "Epoch: 125 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.041 | Train Acc: 98.43%\n",
      "\t Val. Loss: 0.726 |  Val. Acc: 86.98%\n",
      "Epoch: 126 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.045 | Train Acc: 98.15%\n",
      "\t Val. Loss: 0.631 |  Val. Acc: 87.97%\n",
      "Epoch: 127 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.050 | Train Acc: 97.97%\n",
      "\t Val. Loss: 0.671 |  Val. Acc: 86.56%\n",
      "Epoch: 128 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.047 | Train Acc: 98.05%\n",
      "\t Val. Loss: 0.675 |  Val. Acc: 87.52%\n",
      "Epoch: 129 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.049 | Train Acc: 97.92%\n",
      "\t Val. Loss: 0.690 |  Val. Acc: 87.06%\n",
      "Epoch: 130 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.046 | Train Acc: 98.08%\n",
      "\t Val. Loss: 0.726 |  Val. Acc: 87.61%\n",
      "Epoch: 131 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.045 | Train Acc: 98.02%\n",
      "\t Val. Loss: 0.732 |  Val. Acc: 87.56%\n",
      "Epoch: 132 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.055 | Train Acc: 97.61%\n",
      "\t Val. Loss: 0.635 |  Val. Acc: 87.30%\n",
      "Epoch: 133 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.047 | Train Acc: 98.01%\n",
      "\t Val. Loss: 0.691 |  Val. Acc: 86.98%\n",
      "Epoch: 134 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.044 | Train Acc: 98.15%\n",
      "\t Val. Loss: 0.683 |  Val. Acc: 87.26%\n",
      "Epoch: 135 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.044 | Train Acc: 98.17%\n",
      "\t Val. Loss: 0.715 |  Val. Acc: 87.39%\n",
      "Epoch: 136 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.043 | Train Acc: 98.23%\n",
      "\t Val. Loss: 0.638 |  Val. Acc: 87.74%\n",
      "Epoch: 137 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.045 | Train Acc: 97.97%\n",
      "\t Val. Loss: 0.700 |  Val. Acc: 87.67%\n",
      "Epoch: 138 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.054 | Train Acc: 97.77%\n",
      "\t Val. Loss: 0.670 |  Val. Acc: 87.08%\n",
      "Epoch: 139 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.050 | Train Acc: 97.71%\n",
      "\t Val. Loss: 0.655 |  Val. Acc: 87.67%\n",
      "Epoch: 140 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.051 | Train Acc: 98.08%\n",
      "\t Val. Loss: 0.678 |  Val. Acc: 86.98%\n",
      "Epoch: 141 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.045 | Train Acc: 98.08%\n",
      "\t Val. Loss: 0.745 |  Val. Acc: 87.26%\n",
      "Epoch: 142 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.046 | Train Acc: 98.04%\n",
      "\t Val. Loss: 0.709 |  Val. Acc: 87.39%\n",
      "Epoch: 143 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.050 | Train Acc: 97.87%\n",
      "\t Val. Loss: 0.707 |  Val. Acc: 87.82%\n",
      "Epoch: 144 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.041 | Train Acc: 98.02%\n",
      "\t Val. Loss: 0.704 |  Val. Acc: 87.74%\n",
      "Epoch: 145 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.047 | Train Acc: 97.83%\n",
      "\t Val. Loss: 0.736 |  Val. Acc: 87.43%\n",
      "Epoch: 146 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.048 | Train Acc: 98.09%\n",
      "\t Val. Loss: 0.709 |  Val. Acc: 86.93%\n",
      "Epoch: 147 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.051 | Train Acc: 97.83%\n",
      "\t Val. Loss: 0.644 |  Val. Acc: 87.11%\n",
      "Epoch: 148 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.049 | Train Acc: 98.06%\n",
      "\t Val. Loss: 0.677 |  Val. Acc: 87.28%\n",
      "Epoch: 149 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.050 | Train Acc: 97.73%\n",
      "\t Val. Loss: 0.649 |  Val. Acc: 87.17%\n",
      "Epoch: 150 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.044 | Train Acc: 98.02%\n",
      "\t Val. Loss: 0.689 |  Val. Acc: 87.89%\n",
      "Epoch: 151 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.047 | Train Acc: 97.83%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 87.41%\n",
      "Epoch: 152 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.050 | Train Acc: 97.89%\n",
      "\t Val. Loss: 0.689 |  Val. Acc: 86.93%\n",
      "Epoch: 153 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.045 | Train Acc: 98.13%\n",
      "\t Val. Loss: 0.716 |  Val. Acc: 87.43%\n",
      "Epoch: 154 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.045 | Train Acc: 98.14%\n",
      "\t Val. Loss: 0.712 |  Val. Acc: 87.24%\n",
      "Epoch: 155 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.044 | Train Acc: 98.20%\n",
      "\t Val. Loss: 0.719 |  Val. Acc: 86.65%\n",
      "Epoch: 156 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.046 | Train Acc: 98.18%\n",
      "\t Val. Loss: 0.664 |  Val. Acc: 87.00%\n",
      "Epoch: 157 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.041 | Train Acc: 98.11%\n",
      "\t Val. Loss: 0.806 |  Val. Acc: 85.43%\n",
      "Epoch: 158 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.053 | Train Acc: 97.78%\n",
      "\t Val. Loss: 0.627 |  Val. Acc: 87.56%\n",
      "Epoch: 159 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.040 | Train Acc: 98.23%\n",
      "\t Val. Loss: 0.794 |  Val. Acc: 87.48%\n",
      "Epoch: 160 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.049 | Train Acc: 97.96%\n",
      "\t Val. Loss: 0.627 |  Val. Acc: 87.71%\n",
      "Epoch: 161 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.039 | Train Acc: 98.22%\n",
      "\t Val. Loss: 0.675 |  Val. Acc: 87.97%\n",
      "Epoch: 162 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.040 | Train Acc: 98.33%\n",
      "\t Val. Loss: 0.676 |  Val. Acc: 88.21%\n",
      "Epoch: 163 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.040 | Train Acc: 98.16%\n",
      "\t Val. Loss: 0.659 |  Val. Acc: 87.67%\n",
      "Epoch: 164 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.046 | Train Acc: 97.87%\n",
      "\t Val. Loss: 0.689 |  Val. Acc: 87.21%\n",
      "Epoch: 165 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.047 | Train Acc: 98.02%\n",
      "\t Val. Loss: 0.642 |  Val. Acc: 87.63%\n",
      "Epoch: 166 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.040 | Train Acc: 98.11%\n",
      "\t Val. Loss: 0.692 |  Val. Acc: 87.19%\n",
      "Epoch: 167 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.045 | Train Acc: 97.94%\n",
      "\t Val. Loss: 0.696 |  Val. Acc: 87.95%\n",
      "Epoch: 168 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.044 | Train Acc: 98.02%\n",
      "\t Val. Loss: 0.698 |  Val. Acc: 87.67%\n",
      "Epoch: 169 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.048 | Train Acc: 97.88%\n",
      "\t Val. Loss: 0.675 |  Val. Acc: 88.21%\n",
      "Epoch: 170 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.040 | Train Acc: 98.18%\n",
      "\t Val. Loss: 0.653 |  Val. Acc: 87.52%\n",
      "Epoch: 171 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.042 | Train Acc: 98.26%\n",
      "\t Val. Loss: 0.670 |  Val. Acc: 88.21%\n",
      "Epoch: 172 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.041 | Train Acc: 98.30%\n",
      "\t Val. Loss: 0.690 |  Val. Acc: 87.89%\n",
      "Epoch: 173 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.038 | Train Acc: 98.41%\n",
      "\t Val. Loss: 0.733 |  Val. Acc: 87.61%\n",
      "Epoch: 174 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.043 | Train Acc: 98.36%\n",
      "\t Val. Loss: 0.798 |  Val. Acc: 87.93%\n",
      "Epoch: 175 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.050 | Train Acc: 97.80%\n",
      "\t Val. Loss: 0.655 |  Val. Acc: 87.67%\n",
      "Epoch: 176 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.043 | Train Acc: 98.08%\n",
      "\t Val. Loss: 0.666 |  Val. Acc: 88.43%\n",
      "Epoch: 177 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.042 | Train Acc: 98.01%\n",
      "\t Val. Loss: 0.755 |  Val. Acc: 87.52%\n",
      "Epoch: 178 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.050 | Train Acc: 97.76%\n",
      "\t Val. Loss: 0.733 |  Val. Acc: 87.50%\n",
      "Epoch: 179 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.047 | Train Acc: 98.32%\n",
      "\t Val. Loss: 0.743 |  Val. Acc: 87.17%\n",
      "Epoch: 180 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.038 | Train Acc: 98.30%\n",
      "\t Val. Loss: 0.740 |  Val. Acc: 87.17%\n",
      "Epoch: 181 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.38%\n",
      "\t Val. Loss: 0.754 |  Val. Acc: 86.48%\n",
      "Epoch: 182 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.052 | Train Acc: 97.87%\n",
      "\t Val. Loss: 0.719 |  Val. Acc: 86.46%\n",
      "Epoch: 183 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.053 | Train Acc: 97.71%\n",
      "\t Val. Loss: 0.705 |  Val. Acc: 87.76%\n",
      "Epoch: 184 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.054 | Train Acc: 97.71%\n",
      "\t Val. Loss: 0.621 |  Val. Acc: 86.93%\n",
      "Epoch: 185 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.040 | Train Acc: 98.22%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 87.58%\n",
      "Epoch: 186 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.46%\n",
      "\t Val. Loss: 0.729 |  Val. Acc: 86.89%\n",
      "Epoch: 187 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.041 | Train Acc: 98.11%\n",
      "\t Val. Loss: 0.795 |  Val. Acc: 87.69%\n",
      "Epoch: 188 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.046 | Train Acc: 97.95%\n",
      "\t Val. Loss: 0.711 |  Val. Acc: 87.65%\n",
      "Epoch: 189 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.041 | Train Acc: 98.41%\n",
      "\t Val. Loss: 0.770 |  Val. Acc: 85.98%\n",
      "Epoch: 190 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.044 | Train Acc: 98.05%\n",
      "\t Val. Loss: 0.703 |  Val. Acc: 86.63%\n",
      "Epoch: 191 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.042 | Train Acc: 98.01%\n",
      "\t Val. Loss: 0.768 |  Val. Acc: 86.83%\n",
      "Epoch: 192 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.042 | Train Acc: 98.40%\n",
      "\t Val. Loss: 0.735 |  Val. Acc: 87.32%\n",
      "Epoch: 193 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.038 | Train Acc: 98.15%\n",
      "\t Val. Loss: 0.779 |  Val. Acc: 87.39%\n",
      "Epoch: 194 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.045 | Train Acc: 98.10%\n",
      "\t Val. Loss: 0.697 |  Val. Acc: 86.95%\n",
      "Epoch: 195 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.039 | Train Acc: 98.25%\n",
      "\t Val. Loss: 0.737 |  Val. Acc: 86.95%\n",
      "Epoch: 196 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.041 | Train Acc: 98.16%\n",
      "\t Val. Loss: 0.720 |  Val. Acc: 86.89%\n",
      "Epoch: 197 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.038 | Train Acc: 98.41%\n",
      "\t Val. Loss: 0.735 |  Val. Acc: 87.32%\n",
      "Epoch: 198 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.042 | Train Acc: 98.21%\n",
      "\t Val. Loss: 0.748 |  Val. Acc: 86.74%\n",
      "Epoch: 199 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.045 | Train Acc: 98.11%\n",
      "\t Val. Loss: 0.725 |  Val. Acc: 86.91%\n",
      "Epoch: 200 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.043 | Train Acc: 98.08%\n",
      "\t Val. Loss: 0.718 |  Val. Acc: 87.24%\n",
      "Epoch: 201 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.040 | Train Acc: 98.09%\n",
      "\t Val. Loss: 0.774 |  Val. Acc: 87.59%\n",
      "Epoch: 202 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.044 | Train Acc: 98.08%\n",
      "\t Val. Loss: 0.738 |  Val. Acc: 86.65%\n",
      "Epoch: 203 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.040 | Train Acc: 98.37%\n",
      "\t Val. Loss: 0.646 |  Val. Acc: 87.45%\n",
      "Epoch: 204 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.036 | Train Acc: 98.47%\n",
      "\t Val. Loss: 0.704 |  Val. Acc: 87.02%\n",
      "Epoch: 205 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.49%\n",
      "\t Val. Loss: 0.793 |  Val. Acc: 87.87%\n",
      "Epoch: 206 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.042 | Train Acc: 98.13%\n",
      "\t Val. Loss: 0.731 |  Val. Acc: 87.58%\n",
      "Epoch: 207 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.043 | Train Acc: 98.02%\n",
      "\t Val. Loss: 0.674 |  Val. Acc: 87.28%\n",
      "Epoch: 208 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.049 | Train Acc: 97.82%\n",
      "\t Val. Loss: 0.660 |  Val. Acc: 88.26%\n",
      "Epoch: 209 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.041 | Train Acc: 98.22%\n",
      "\t Val. Loss: 0.688 |  Val. Acc: 87.80%\n",
      "Epoch: 210 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.044 | Train Acc: 97.97%\n",
      "\t Val. Loss: 0.682 |  Val. Acc: 88.19%\n",
      "Epoch: 211 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.040 | Train Acc: 98.34%\n",
      "\t Val. Loss: 0.727 |  Val. Acc: 87.43%\n",
      "Epoch: 212 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.036 | Train Acc: 98.62%\n",
      "\t Val. Loss: 0.746 |  Val. Acc: 86.69%\n",
      "Epoch: 213 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.040 | Train Acc: 98.33%\n",
      "\t Val. Loss: 0.688 |  Val. Acc: 87.35%\n",
      "Epoch: 214 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.047 | Train Acc: 98.05%\n",
      "\t Val. Loss: 0.716 |  Val. Acc: 88.08%\n",
      "Epoch: 215 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.044 | Train Acc: 98.25%\n",
      "\t Val. Loss: 0.626 |  Val. Acc: 87.54%\n",
      "Epoch: 216 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.040 | Train Acc: 98.45%\n",
      "\t Val. Loss: 0.698 |  Val. Acc: 87.30%\n",
      "Epoch: 217 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.044 | Train Acc: 98.06%\n",
      "\t Val. Loss: 0.732 |  Val. Acc: 87.06%\n",
      "Epoch: 218 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.039 | Train Acc: 98.16%\n",
      "\t Val. Loss: 0.660 |  Val. Acc: 87.76%\n",
      "Epoch: 219 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.035 | Train Acc: 98.55%\n",
      "\t Val. Loss: 0.702 |  Val. Acc: 87.50%\n",
      "Epoch: 220 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.039 | Train Acc: 98.17%\n",
      "\t Val. Loss: 0.720 |  Val. Acc: 88.02%\n",
      "Epoch: 221 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.041 | Train Acc: 98.18%\n",
      "\t Val. Loss: 0.648 |  Val. Acc: 87.95%\n",
      "Epoch: 222 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.036 | Train Acc: 98.44%\n",
      "\t Val. Loss: 0.744 |  Val. Acc: 87.97%\n",
      "Epoch: 223 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.046 | Train Acc: 98.03%\n",
      "\t Val. Loss: 0.660 |  Val. Acc: 88.32%\n",
      "Epoch: 224 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.044 | Train Acc: 97.93%\n",
      "\t Val. Loss: 0.740 |  Val. Acc: 87.35%\n",
      "Epoch: 225 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.042 | Train Acc: 98.17%\n",
      "\t Val. Loss: 0.740 |  Val. Acc: 87.08%\n",
      "Epoch: 226 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.041 | Train Acc: 98.30%\n",
      "\t Val. Loss: 0.705 |  Val. Acc: 87.69%\n",
      "Epoch: 227 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.040 | Train Acc: 98.18%\n",
      "\t Val. Loss: 0.715 |  Val. Acc: 87.73%\n",
      "Epoch: 228 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.040 | Train Acc: 98.15%\n",
      "\t Val. Loss: 0.731 |  Val. Acc: 87.02%\n",
      "Epoch: 229 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.038 | Train Acc: 98.30%\n",
      "\t Val. Loss: 0.779 |  Val. Acc: 87.78%\n",
      "Epoch: 230 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.043 | Train Acc: 98.10%\n",
      "\t Val. Loss: 0.763 |  Val. Acc: 87.76%\n",
      "Epoch: 231 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.039 | Train Acc: 98.13%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 87.30%\n",
      "Epoch: 232 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.042 | Train Acc: 98.36%\n",
      "\t Val. Loss: 0.745 |  Val. Acc: 87.76%\n",
      "Epoch: 233 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.045 | Train Acc: 97.99%\n",
      "\t Val. Loss: 0.660 |  Val. Acc: 88.19%\n",
      "Epoch: 234 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.040 | Train Acc: 98.23%\n",
      "\t Val. Loss: 0.723 |  Val. Acc: 87.84%\n",
      "Epoch: 235 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.042 | Train Acc: 98.18%\n",
      "\t Val. Loss: 0.736 |  Val. Acc: 87.67%\n",
      "Epoch: 236 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.050 | Train Acc: 97.94%\n",
      "\t Val. Loss: 0.626 |  Val. Acc: 86.69%\n",
      "Epoch: 237 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.044 | Train Acc: 97.96%\n",
      "\t Val. Loss: 0.766 |  Val. Acc: 87.41%\n",
      "Epoch: 238 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.043 | Train Acc: 97.92%\n",
      "\t Val. Loss: 0.711 |  Val. Acc: 88.02%\n",
      "Epoch: 239 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.040 | Train Acc: 98.29%\n",
      "\t Val. Loss: 0.758 |  Val. Acc: 87.95%\n",
      "Epoch: 240 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.047 | Train Acc: 98.14%\n",
      "\t Val. Loss: 0.687 |  Val. Acc: 87.24%\n",
      "Epoch: 241 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.049 | Train Acc: 97.88%\n",
      "\t Val. Loss: 0.710 |  Val. Acc: 87.13%\n",
      "Epoch: 242 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.039 | Train Acc: 98.37%\n",
      "\t Val. Loss: 0.776 |  Val. Acc: 87.56%\n",
      "Epoch: 243 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.040 | Train Acc: 98.27%\n",
      "\t Val. Loss: 0.759 |  Val. Acc: 87.50%\n",
      "Epoch: 244 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.042 | Train Acc: 98.18%\n",
      "\t Val. Loss: 0.829 |  Val. Acc: 88.00%\n",
      "Epoch: 245 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.039 | Train Acc: 98.28%\n",
      "\t Val. Loss: 0.717 |  Val. Acc: 86.85%\n",
      "Epoch: 246 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.048 | Train Acc: 98.06%\n",
      "\t Val. Loss: 0.738 |  Val. Acc: 87.13%\n",
      "Epoch: 247 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.043 | Train Acc: 97.94%\n",
      "\t Val. Loss: 0.702 |  Val. Acc: 87.21%\n",
      "Epoch: 248 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.038 | Train Acc: 98.26%\n",
      "\t Val. Loss: 0.718 |  Val. Acc: 87.48%\n",
      "Epoch: 249 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.036 | Train Acc: 98.56%\n",
      "\t Val. Loss: 0.727 |  Val. Acc: 87.65%\n",
      "Epoch: 250 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.034 | Train Acc: 98.68%\n",
      "\t Val. Loss: 0.775 |  Val. Acc: 87.56%\n",
      "Epoch: 251 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.038 | Train Acc: 98.33%\n",
      "\t Val. Loss: 0.751 |  Val. Acc: 87.65%\n",
      "Epoch: 252 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.036 | Train Acc: 98.11%\n",
      "\t Val. Loss: 0.797 |  Val. Acc: 87.69%\n",
      "Epoch: 253 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.040 | Train Acc: 98.32%\n",
      "\t Val. Loss: 0.776 |  Val. Acc: 87.78%\n",
      "Epoch: 254 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.040 | Train Acc: 98.18%\n",
      "\t Val. Loss: 0.761 |  Val. Acc: 87.91%\n",
      "Epoch: 255 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.032 | Train Acc: 98.59%\n",
      "\t Val. Loss: 0.784 |  Val. Acc: 87.21%\n",
      "Epoch: 256 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.040 | Train Acc: 98.09%\n",
      "\t Val. Loss: 0.768 |  Val. Acc: 87.26%\n",
      "Epoch: 257 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.045 | Train Acc: 97.99%\n",
      "\t Val. Loss: 0.670 |  Val. Acc: 87.39%\n",
      "Epoch: 258 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.036 | Train Acc: 98.34%\n",
      "\t Val. Loss: 0.769 |  Val. Acc: 87.19%\n",
      "Epoch: 259 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.042 | Train Acc: 98.04%\n",
      "\t Val. Loss: 0.733 |  Val. Acc: 86.74%\n",
      "Epoch: 260 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.046 | Train Acc: 98.03%\n",
      "\t Val. Loss: 0.719 |  Val. Acc: 88.11%\n",
      "Epoch: 261 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.042 | Train Acc: 98.13%\n",
      "\t Val. Loss: 0.683 |  Val. Acc: 87.95%\n",
      "Epoch: 262 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.043 | Train Acc: 98.16%\n",
      "\t Val. Loss: 0.669 |  Val. Acc: 87.97%\n",
      "Epoch: 263 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.041 | Train Acc: 98.21%\n",
      "\t Val. Loss: 0.688 |  Val. Acc: 87.30%\n",
      "Epoch: 264 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.046 | Train Acc: 98.13%\n",
      "\t Val. Loss: 0.662 |  Val. Acc: 86.74%\n",
      "Epoch: 265 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.043 | Train Acc: 97.97%\n",
      "\t Val. Loss: 0.670 |  Val. Acc: 87.19%\n",
      "Epoch: 266 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.041 | Train Acc: 98.39%\n",
      "\t Val. Loss: 0.647 |  Val. Acc: 86.74%\n",
      "Epoch: 267 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.041 | Train Acc: 98.29%\n",
      "\t Val. Loss: 0.720 |  Val. Acc: 87.69%\n",
      "Epoch: 268 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.038 | Train Acc: 98.14%\n",
      "\t Val. Loss: 0.707 |  Val. Acc: 86.78%\n",
      "Epoch: 269 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.043 | Train Acc: 98.21%\n",
      "\t Val. Loss: 0.641 |  Val. Acc: 87.72%\n",
      "Epoch: 270 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.036 | Train Acc: 98.38%\n",
      "\t Val. Loss: 0.661 |  Val. Acc: 87.58%\n",
      "Epoch: 271 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.32%\n",
      "\t Val. Loss: 0.773 |  Val. Acc: 86.33%\n",
      "Epoch: 272 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.042 | Train Acc: 98.10%\n",
      "\t Val. Loss: 0.729 |  Val. Acc: 87.74%\n",
      "Epoch: 273 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.045 | Train Acc: 97.97%\n",
      "\t Val. Loss: 0.675 |  Val. Acc: 87.67%\n",
      "Epoch: 274 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.36%\n",
      "\t Val. Loss: 0.704 |  Val. Acc: 87.84%\n",
      "Epoch: 275 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.038 | Train Acc: 98.27%\n",
      "\t Val. Loss: 0.695 |  Val. Acc: 87.91%\n",
      "Epoch: 276 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.44%\n",
      "\t Val. Loss: 0.713 |  Val. Acc: 87.17%\n",
      "Epoch: 277 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.042 | Train Acc: 98.31%\n",
      "\t Val. Loss: 0.658 |  Val. Acc: 87.56%\n",
      "Epoch: 278 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.038 | Train Acc: 98.42%\n",
      "\t Val. Loss: 0.671 |  Val. Acc: 87.52%\n",
      "Epoch: 279 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.033 | Train Acc: 98.52%\n",
      "\t Val. Loss: 0.741 |  Val. Acc: 87.67%\n",
      "Epoch: 280 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.033 | Train Acc: 98.57%\n",
      "\t Val. Loss: 0.764 |  Val. Acc: 87.43%\n",
      "Epoch: 281 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.039 | Train Acc: 98.29%\n",
      "\t Val. Loss: 0.735 |  Val. Acc: 87.93%\n",
      "Epoch: 282 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.034 | Train Acc: 98.46%\n",
      "\t Val. Loss: 0.746 |  Val. Acc: 87.85%\n",
      "Epoch: 283 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.039 | Train Acc: 98.34%\n",
      "\t Val. Loss: 0.727 |  Val. Acc: 87.78%\n",
      "Epoch: 284 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.045 | Train Acc: 98.26%\n",
      "\t Val. Loss: 0.683 |  Val. Acc: 87.41%\n",
      "Epoch: 285 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.34%\n",
      "\t Val. Loss: 0.727 |  Val. Acc: 87.30%\n",
      "Epoch: 286 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.041 | Train Acc: 98.29%\n",
      "\t Val. Loss: 0.752 |  Val. Acc: 86.98%\n",
      "Epoch: 287 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.032 | Train Acc: 98.51%\n",
      "\t Val. Loss: 0.756 |  Val. Acc: 87.72%\n",
      "Epoch: 288 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.039 | Train Acc: 98.32%\n",
      "\t Val. Loss: 0.789 |  Val. Acc: 87.91%\n",
      "Epoch: 289 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.052 | Train Acc: 97.80%\n",
      "\t Val. Loss: 0.697 |  Val. Acc: 87.45%\n",
      "Epoch: 290 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.032 | Train Acc: 98.56%\n",
      "\t Val. Loss: 0.805 |  Val. Acc: 88.04%\n",
      "Epoch: 291 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.22%\n",
      "\t Val. Loss: 0.756 |  Val. Acc: 88.08%\n",
      "Epoch: 292 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.040 | Train Acc: 98.18%\n",
      "\t Val. Loss: 0.710 |  Val. Acc: 87.65%\n",
      "Epoch: 293 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.040 | Train Acc: 98.28%\n",
      "\t Val. Loss: 0.729 |  Val. Acc: 86.72%\n",
      "Epoch: 294 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.038 | Train Acc: 98.46%\n",
      "\t Val. Loss: 0.691 |  Val. Acc: 87.69%\n",
      "Epoch: 295 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.040 | Train Acc: 98.16%\n",
      "\t Val. Loss: 0.789 |  Val. Acc: 87.58%\n",
      "Epoch: 296 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.042 | Train Acc: 98.09%\n",
      "\t Val. Loss: 0.735 |  Val. Acc: 87.19%\n",
      "Epoch: 297 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.23%\n",
      "\t Val. Loss: 0.837 |  Val. Acc: 86.80%\n",
      "Epoch: 298 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.32%\n",
      "\t Val. Loss: 0.719 |  Val. Acc: 87.45%\n",
      "Epoch: 299 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.035 | Train Acc: 98.30%\n",
      "\t Val. Loss: 0.707 |  Val. Acc: 87.48%\n",
      "Epoch: 300 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.038 | Train Acc: 98.20%\n",
      "\t Val. Loss: 0.811 |  Val. Acc: 88.13%\n",
      "Epoch: 301 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.035 | Train Acc: 98.39%\n",
      "\t Val. Loss: 0.769 |  Val. Acc: 87.69%\n",
      "Epoch: 302 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.036 | Train Acc: 98.35%\n",
      "\t Val. Loss: 0.728 |  Val. Acc: 87.13%\n",
      "Epoch: 303 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.036 | Train Acc: 98.39%\n",
      "\t Val. Loss: 0.740 |  Val. Acc: 86.93%\n",
      "Epoch: 304 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.35%\n",
      "\t Val. Loss: 0.768 |  Val. Acc: 87.52%\n",
      "Epoch: 305 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.041 | Train Acc: 98.09%\n",
      "\t Val. Loss: 0.689 |  Val. Acc: 87.84%\n",
      "Epoch: 306 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.039 | Train Acc: 98.34%\n",
      "\t Val. Loss: 0.648 |  Val. Acc: 88.30%\n",
      "Epoch: 307 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.043 | Train Acc: 97.92%\n",
      "\t Val. Loss: 0.719 |  Val. Acc: 87.82%\n",
      "Epoch: 308 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.034 | Train Acc: 98.34%\n",
      "\t Val. Loss: 0.718 |  Val. Acc: 87.71%\n",
      "Epoch: 309 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.31%\n",
      "\t Val. Loss: 0.703 |  Val. Acc: 87.43%\n",
      "Epoch: 310 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.032 | Train Acc: 98.55%\n",
      "\t Val. Loss: 0.764 |  Val. Acc: 88.21%\n",
      "Epoch: 311 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.035 | Train Acc: 98.41%\n",
      "\t Val. Loss: 0.740 |  Val. Acc: 87.21%\n",
      "Epoch: 312 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.043 | Train Acc: 97.99%\n",
      "\t Val. Loss: 0.732 |  Val. Acc: 86.28%\n",
      "Epoch: 313 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.044 | Train Acc: 98.04%\n",
      "\t Val. Loss: 0.719 |  Val. Acc: 87.84%\n",
      "Epoch: 314 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.039 | Train Acc: 98.31%\n",
      "\t Val. Loss: 0.696 |  Val. Acc: 87.54%\n",
      "Epoch: 315 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.40%\n",
      "\t Val. Loss: 0.722 |  Val. Acc: 88.11%\n",
      "Epoch: 316 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.035 | Train Acc: 98.57%\n",
      "\t Val. Loss: 0.718 |  Val. Acc: 86.56%\n",
      "Epoch: 317 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.034 | Train Acc: 98.36%\n",
      "\t Val. Loss: 0.778 |  Val. Acc: 87.61%\n",
      "Epoch: 318 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.045 | Train Acc: 98.03%\n",
      "\t Val. Loss: 0.654 |  Val. Acc: 87.78%\n",
      "Epoch: 319 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.038 | Train Acc: 98.26%\n",
      "\t Val. Loss: 0.748 |  Val. Acc: 87.52%\n",
      "Epoch: 320 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.040 | Train Acc: 98.24%\n",
      "\t Val. Loss: 0.726 |  Val. Acc: 87.17%\n",
      "Epoch: 321 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.34%\n",
      "\t Val. Loss: 0.782 |  Val. Acc: 86.69%\n",
      "Epoch: 322 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.38%\n",
      "\t Val. Loss: 0.766 |  Val. Acc: 87.24%\n",
      "Epoch: 323 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.035 | Train Acc: 98.39%\n",
      "\t Val. Loss: 0.756 |  Val. Acc: 87.65%\n",
      "Epoch: 324 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.040 | Train Acc: 98.32%\n",
      "\t Val. Loss: 0.709 |  Val. Acc: 88.08%\n",
      "Epoch: 325 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.042 | Train Acc: 98.09%\n",
      "\t Val. Loss: 0.705 |  Val. Acc: 87.58%\n",
      "Epoch: 326 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.20%\n",
      "\t Val. Loss: 0.784 |  Val. Acc: 87.50%\n",
      "Epoch: 327 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.033 | Train Acc: 98.65%\n",
      "\t Val. Loss: 0.808 |  Val. Acc: 87.15%\n",
      "Epoch: 328 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.038 | Train Acc: 98.41%\n",
      "\t Val. Loss: 0.822 |  Val. Acc: 88.04%\n",
      "Epoch: 329 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.036 | Train Acc: 98.42%\n",
      "\t Val. Loss: 0.732 |  Val. Acc: 87.06%\n",
      "Epoch: 330 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.035 | Train Acc: 98.48%\n",
      "\t Val. Loss: 0.761 |  Val. Acc: 87.67%\n",
      "Epoch: 331 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.49%\n",
      "\t Val. Loss: 0.705 |  Val. Acc: 87.84%\n",
      "Epoch: 332 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.038 | Train Acc: 98.30%\n",
      "\t Val. Loss: 0.742 |  Val. Acc: 87.69%\n",
      "Epoch: 333 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.50%\n",
      "\t Val. Loss: 0.764 |  Val. Acc: 88.02%\n",
      "Epoch: 334 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.041 | Train Acc: 98.23%\n",
      "\t Val. Loss: 0.718 |  Val. Acc: 87.63%\n",
      "Epoch: 335 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.38%\n",
      "\t Val. Loss: 0.761 |  Val. Acc: 87.63%\n",
      "Epoch: 336 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.044 | Train Acc: 97.84%\n",
      "\t Val. Loss: 0.729 |  Val. Acc: 87.00%\n",
      "Epoch: 337 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.041 | Train Acc: 98.24%\n",
      "\t Val. Loss: 0.660 |  Val. Acc: 87.97%\n",
      "Epoch: 338 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.041 | Train Acc: 98.18%\n",
      "\t Val. Loss: 0.763 |  Val. Acc: 87.58%\n",
      "Epoch: 339 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.038 | Train Acc: 98.42%\n",
      "\t Val. Loss: 0.670 |  Val. Acc: 87.95%\n",
      "Epoch: 340 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.040 | Train Acc: 98.17%\n",
      "\t Val. Loss: 0.764 |  Val. Acc: 87.89%\n",
      "Epoch: 341 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.036 | Train Acc: 98.34%\n",
      "\t Val. Loss: 0.765 |  Val. Acc: 87.89%\n",
      "Epoch: 342 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.040 | Train Acc: 98.14%\n",
      "\t Val. Loss: 0.755 |  Val. Acc: 87.95%\n",
      "Epoch: 343 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.22%\n",
      "\t Val. Loss: 0.774 |  Val. Acc: 86.61%\n",
      "Epoch: 344 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.042 | Train Acc: 98.30%\n",
      "\t Val. Loss: 0.708 |  Val. Acc: 87.91%\n",
      "Epoch: 345 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.040 | Train Acc: 98.25%\n",
      "\t Val. Loss: 0.710 |  Val. Acc: 87.56%\n",
      "Epoch: 346 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.043 | Train Acc: 98.27%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 87.78%\n",
      "Epoch: 347 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.036 | Train Acc: 98.32%\n",
      "\t Val. Loss: 0.768 |  Val. Acc: 87.76%\n",
      "Epoch: 348 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.039 | Train Acc: 98.36%\n",
      "\t Val. Loss: 0.723 |  Val. Acc: 86.48%\n",
      "Epoch: 349 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.45%\n",
      "\t Val. Loss: 0.703 |  Val. Acc: 87.71%\n",
      "Epoch: 350 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.034 | Train Acc: 98.23%\n",
      "\t Val. Loss: 0.764 |  Val. Acc: 87.69%\n",
      "Epoch: 351 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.041 | Train Acc: 98.11%\n",
      "\t Val. Loss: 0.658 |  Val. Acc: 88.06%\n",
      "Epoch: 352 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.042 | Train Acc: 98.18%\n",
      "\t Val. Loss: 0.777 |  Val. Acc: 87.93%\n",
      "Epoch: 353 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.036 | Train Acc: 98.27%\n",
      "\t Val. Loss: 0.728 |  Val. Acc: 87.52%\n",
      "Epoch: 354 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.041 | Train Acc: 98.09%\n",
      "\t Val. Loss: 0.660 |  Val. Acc: 87.67%\n",
      "Epoch: 355 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.042 | Train Acc: 97.96%\n",
      "\t Val. Loss: 0.638 |  Val. Acc: 87.11%\n",
      "Epoch: 356 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.042 | Train Acc: 97.96%\n",
      "\t Val. Loss: 0.760 |  Val. Acc: 87.00%\n",
      "Epoch: 357 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.17%\n",
      "\t Val. Loss: 0.739 |  Val. Acc: 87.08%\n",
      "Epoch: 358 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.033 | Train Acc: 98.52%\n",
      "\t Val. Loss: 0.746 |  Val. Acc: 87.30%\n",
      "Epoch: 359 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.032 | Train Acc: 98.49%\n",
      "\t Val. Loss: 0.749 |  Val. Acc: 86.98%\n",
      "Epoch: 360 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.039 | Train Acc: 98.35%\n",
      "\t Val. Loss: 0.733 |  Val. Acc: 87.17%\n",
      "Epoch: 361 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.035 | Train Acc: 98.51%\n",
      "\t Val. Loss: 0.742 |  Val. Acc: 87.08%\n",
      "Epoch: 362 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.038 | Train Acc: 98.24%\n",
      "\t Val. Loss: 0.820 |  Val. Acc: 86.19%\n",
      "Epoch: 363 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.28%\n",
      "\t Val. Loss: 0.823 |  Val. Acc: 87.48%\n",
      "Epoch: 364 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.034 | Train Acc: 98.34%\n",
      "\t Val. Loss: 0.814 |  Val. Acc: 87.26%\n",
      "Epoch: 365 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.032 | Train Acc: 98.67%\n",
      "\t Val. Loss: 0.728 |  Val. Acc: 87.43%\n",
      "Epoch: 366 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.040 | Train Acc: 98.20%\n",
      "\t Val. Loss: 0.758 |  Val. Acc: 87.63%\n",
      "Epoch: 367 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.036 | Train Acc: 98.30%\n",
      "\t Val. Loss: 0.788 |  Val. Acc: 87.15%\n",
      "Epoch: 368 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.041 | Train Acc: 98.22%\n",
      "\t Val. Loss: 0.667 |  Val. Acc: 87.11%\n",
      "Epoch: 369 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.036 | Train Acc: 98.48%\n",
      "\t Val. Loss: 0.732 |  Val. Acc: 86.85%\n",
      "Epoch: 370 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.036 | Train Acc: 98.31%\n",
      "\t Val. Loss: 0.770 |  Val. Acc: 87.82%\n",
      "Epoch: 371 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.033 | Train Acc: 98.53%\n",
      "\t Val. Loss: 0.743 |  Val. Acc: 88.34%\n",
      "Epoch: 372 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.038 | Train Acc: 98.35%\n",
      "\t Val. Loss: 0.760 |  Val. Acc: 87.74%\n",
      "Epoch: 373 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.036 | Train Acc: 98.30%\n",
      "\t Val. Loss: 0.820 |  Val. Acc: 87.82%\n",
      "Epoch: 374 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.038 | Train Acc: 98.18%\n",
      "\t Val. Loss: 0.793 |  Val. Acc: 87.65%\n",
      "Epoch: 375 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.038 | Train Acc: 98.33%\n",
      "\t Val. Loss: 0.822 |  Val. Acc: 87.48%\n",
      "Epoch: 376 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.28%\n",
      "\t Val. Loss: 0.755 |  Val. Acc: 88.00%\n",
      "Epoch: 377 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.042 | Train Acc: 98.05%\n",
      "\t Val. Loss: 0.679 |  Val. Acc: 88.21%\n",
      "Epoch: 378 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.039 | Train Acc: 98.26%\n",
      "\t Val. Loss: 0.706 |  Val. Acc: 87.84%\n",
      "Epoch: 379 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.036 | Train Acc: 98.37%\n",
      "\t Val. Loss: 0.689 |  Val. Acc: 87.91%\n",
      "Epoch: 380 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.036 | Train Acc: 98.60%\n",
      "\t Val. Loss: 0.707 |  Val. Acc: 87.13%\n",
      "Epoch: 381 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.033 | Train Acc: 98.46%\n",
      "\t Val. Loss: 0.733 |  Val. Acc: 87.28%\n",
      "Epoch: 382 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.035 | Train Acc: 98.36%\n",
      "\t Val. Loss: 0.771 |  Val. Acc: 87.63%\n",
      "Epoch: 383 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.036 | Train Acc: 98.34%\n",
      "\t Val. Loss: 0.748 |  Val. Acc: 86.85%\n",
      "Epoch: 384 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.036 | Train Acc: 98.39%\n",
      "\t Val. Loss: 0.753 |  Val. Acc: 87.32%\n",
      "Epoch: 385 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.033 | Train Acc: 98.36%\n",
      "\t Val. Loss: 0.792 |  Val. Acc: 87.52%\n",
      "Epoch: 386 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.041 | Train Acc: 98.18%\n",
      "\t Val. Loss: 0.741 |  Val. Acc: 88.04%\n",
      "Epoch: 387 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.034 | Train Acc: 98.57%\n",
      "\t Val. Loss: 0.806 |  Val. Acc: 87.76%\n",
      "Epoch: 388 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.033 | Train Acc: 98.51%\n",
      "\t Val. Loss: 0.806 |  Val. Acc: 87.24%\n",
      "Epoch: 389 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.42%\n",
      "\t Val. Loss: 0.793 |  Val. Acc: 87.30%\n",
      "Epoch: 390 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.30%\n",
      "\t Val. Loss: 0.792 |  Val. Acc: 87.37%\n",
      "Epoch: 391 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.035 | Train Acc: 98.51%\n",
      "\t Val. Loss: 0.888 |  Val. Acc: 86.67%\n",
      "Epoch: 392 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.039 | Train Acc: 98.47%\n",
      "\t Val. Loss: 0.745 |  Val. Acc: 86.85%\n",
      "Epoch: 393 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.59%\n",
      "\t Val. Loss: 0.744 |  Val. Acc: 87.06%\n",
      "Epoch: 394 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.039 | Train Acc: 98.30%\n",
      "\t Val. Loss: 0.705 |  Val. Acc: 87.08%\n",
      "Epoch: 395 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.033 | Train Acc: 98.65%\n",
      "\t Val. Loss: 0.807 |  Val. Acc: 87.19%\n",
      "Epoch: 396 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.31%\n",
      "\t Val. Loss: 0.711 |  Val. Acc: 86.89%\n",
      "Epoch: 397 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.033 | Train Acc: 98.34%\n",
      "\t Val. Loss: 0.759 |  Val. Acc: 86.83%\n",
      "Epoch: 398 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.039 | Train Acc: 98.25%\n",
      "\t Val. Loss: 0.735 |  Val. Acc: 87.30%\n",
      "Epoch: 399 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.09%\n",
      "\t Val. Loss: 0.761 |  Val. Acc: 86.69%\n",
      "Epoch: 400 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.031 | Train Acc: 98.54%\n",
      "\t Val. Loss: 0.769 |  Val. Acc: 86.67%\n",
      "Epoch: 401 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.045 | Train Acc: 98.10%\n",
      "\t Val. Loss: 0.740 |  Val. Acc: 87.17%\n",
      "Epoch: 402 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.32%\n",
      "\t Val. Loss: 0.773 |  Val. Acc: 87.80%\n",
      "Epoch: 403 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.039 | Train Acc: 98.17%\n",
      "\t Val. Loss: 0.743 |  Val. Acc: 87.15%\n",
      "Epoch: 404 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.035 | Train Acc: 98.55%\n",
      "\t Val. Loss: 0.700 |  Val. Acc: 87.93%\n",
      "Epoch: 405 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.27%\n",
      "\t Val. Loss: 0.744 |  Val. Acc: 87.11%\n",
      "Epoch: 406 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.035 | Train Acc: 98.42%\n",
      "\t Val. Loss: 0.744 |  Val. Acc: 88.02%\n",
      "Epoch: 407 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.038 | Train Acc: 98.13%\n",
      "\t Val. Loss: 0.691 |  Val. Acc: 86.87%\n",
      "Epoch: 408 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.031 | Train Acc: 98.52%\n",
      "\t Val. Loss: 0.733 |  Val. Acc: 87.48%\n",
      "Epoch: 409 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.033 | Train Acc: 98.65%\n",
      "\t Val. Loss: 0.793 |  Val. Acc: 87.39%\n",
      "Epoch: 410 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.035 | Train Acc: 98.49%\n",
      "\t Val. Loss: 0.740 |  Val. Acc: 87.06%\n",
      "Epoch: 411 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.035 | Train Acc: 98.46%\n",
      "\t Val. Loss: 0.801 |  Val. Acc: 87.15%\n",
      "Epoch: 412 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.038 | Train Acc: 98.29%\n",
      "\t Val. Loss: 0.795 |  Val. Acc: 86.63%\n",
      "Epoch: 413 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.033 | Train Acc: 98.36%\n",
      "\t Val. Loss: 0.790 |  Val. Acc: 87.13%\n",
      "Epoch: 414 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.034 | Train Acc: 98.37%\n",
      "\t Val. Loss: 0.745 |  Val. Acc: 86.65%\n",
      "Epoch: 415 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.033 | Train Acc: 98.44%\n",
      "\t Val. Loss: 0.759 |  Val. Acc: 87.02%\n",
      "Epoch: 416 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.032 | Train Acc: 98.42%\n",
      "\t Val. Loss: 0.806 |  Val. Acc: 86.91%\n",
      "Epoch: 417 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.033 | Train Acc: 98.46%\n",
      "\t Val. Loss: 0.765 |  Val. Acc: 87.28%\n",
      "Epoch: 418 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.036 | Train Acc: 98.28%\n",
      "\t Val. Loss: 0.780 |  Val. Acc: 87.17%\n",
      "Epoch: 419 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.17%\n",
      "\t Val. Loss: 0.806 |  Val. Acc: 86.80%\n",
      "Epoch: 420 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.033 | Train Acc: 98.49%\n",
      "\t Val. Loss: 0.799 |  Val. Acc: 86.65%\n",
      "Epoch: 421 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.039 | Train Acc: 98.29%\n",
      "\t Val. Loss: 0.776 |  Val. Acc: 87.37%\n",
      "Epoch: 422 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.040 | Train Acc: 98.25%\n",
      "\t Val. Loss: 0.735 |  Val. Acc: 86.98%\n",
      "Epoch: 423 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.036 | Train Acc: 98.31%\n",
      "\t Val. Loss: 0.707 |  Val. Acc: 87.19%\n",
      "Epoch: 424 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.48%\n",
      "\t Val. Loss: 0.772 |  Val. Acc: 86.93%\n",
      "Epoch: 425 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.032 | Train Acc: 98.59%\n",
      "\t Val. Loss: 0.810 |  Val. Acc: 87.11%\n",
      "Epoch: 426 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.036 | Train Acc: 98.30%\n",
      "\t Val. Loss: 0.774 |  Val. Acc: 87.08%\n",
      "Epoch: 427 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.031 | Train Acc: 98.69%\n",
      "\t Val. Loss: 0.753 |  Val. Acc: 87.17%\n",
      "Epoch: 428 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.033 | Train Acc: 98.42%\n",
      "\t Val. Loss: 0.779 |  Val. Acc: 87.43%\n",
      "Epoch: 429 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.034 | Train Acc: 98.57%\n",
      "\t Val. Loss: 0.753 |  Val. Acc: 87.56%\n",
      "Epoch: 430 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.49%\n",
      "\t Val. Loss: 0.721 |  Val. Acc: 87.41%\n",
      "Epoch: 431 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.040 | Train Acc: 98.20%\n",
      "\t Val. Loss: 0.733 |  Val. Acc: 88.24%\n",
      "Epoch: 432 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.042 | Train Acc: 98.06%\n",
      "\t Val. Loss: 0.721 |  Val. Acc: 87.54%\n",
      "Epoch: 433 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.036 | Train Acc: 98.39%\n",
      "\t Val. Loss: 0.748 |  Val. Acc: 87.69%\n",
      "Epoch: 434 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.034 | Train Acc: 98.43%\n",
      "\t Val. Loss: 0.773 |  Val. Acc: 87.78%\n",
      "Epoch: 435 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.033 | Train Acc: 98.36%\n",
      "\t Val. Loss: 0.785 |  Val. Acc: 86.82%\n",
      "Epoch: 436 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.034 | Train Acc: 98.34%\n",
      "\t Val. Loss: 0.761 |  Val. Acc: 87.61%\n",
      "Epoch: 437 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.034 | Train Acc: 98.56%\n",
      "\t Val. Loss: 0.733 |  Val. Acc: 87.24%\n",
      "Epoch: 438 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.034 | Train Acc: 98.60%\n",
      "\t Val. Loss: 0.703 |  Val. Acc: 87.24%\n",
      "Epoch: 439 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.031 | Train Acc: 98.52%\n",
      "\t Val. Loss: 0.761 |  Val. Acc: 86.46%\n",
      "Epoch: 440 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.039 | Train Acc: 98.18%\n",
      "\t Val. Loss: 0.766 |  Val. Acc: 87.22%\n",
      "Epoch: 441 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.036 | Train Acc: 98.41%\n",
      "\t Val. Loss: 0.710 |  Val. Acc: 87.41%\n",
      "Epoch: 442 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.035 | Train Acc: 98.32%\n",
      "\t Val. Loss: 0.759 |  Val. Acc: 87.41%\n",
      "Epoch: 443 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.035 | Train Acc: 98.55%\n",
      "\t Val. Loss: 0.757 |  Val. Acc: 87.32%\n",
      "Epoch: 444 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.034 | Train Acc: 98.48%\n",
      "\t Val. Loss: 0.793 |  Val. Acc: 87.04%\n",
      "Epoch: 445 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.030 | Train Acc: 98.68%\n",
      "\t Val. Loss: 0.794 |  Val. Acc: 86.69%\n",
      "Epoch: 446 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.041 | Train Acc: 98.26%\n",
      "\t Val. Loss: 0.703 |  Val. Acc: 87.41%\n",
      "Epoch: 447 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.034 | Train Acc: 98.41%\n",
      "\t Val. Loss: 0.738 |  Val. Acc: 86.95%\n",
      "Epoch: 448 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.030 | Train Acc: 98.68%\n",
      "\t Val. Loss: 0.776 |  Val. Acc: 86.74%\n",
      "Epoch: 449 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.036 | Train Acc: 98.49%\n",
      "\t Val. Loss: 0.801 |  Val. Acc: 87.21%\n",
      "Epoch: 450 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.041 | Train Acc: 98.31%\n",
      "\t Val. Loss: 0.812 |  Val. Acc: 87.65%\n",
      "Epoch: 451 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.034 | Train Acc: 98.57%\n",
      "\t Val. Loss: 0.768 |  Val. Acc: 87.58%\n",
      "Epoch: 452 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.032 | Train Acc: 98.70%\n",
      "\t Val. Loss: 0.808 |  Val. Acc: 87.08%\n",
      "Epoch: 453 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.036 | Train Acc: 98.27%\n",
      "\t Val. Loss: 0.733 |  Val. Acc: 87.74%\n",
      "Epoch: 454 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.036 | Train Acc: 98.37%\n",
      "\t Val. Loss: 0.716 |  Val. Acc: 87.67%\n",
      "Epoch: 455 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.039 | Train Acc: 98.39%\n",
      "\t Val. Loss: 0.665 |  Val. Acc: 87.76%\n",
      "Epoch: 456 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.031 | Train Acc: 98.71%\n",
      "\t Val. Loss: 0.756 |  Val. Acc: 87.45%\n",
      "Epoch: 457 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.032 | Train Acc: 98.39%\n",
      "\t Val. Loss: 0.754 |  Val. Acc: 87.61%\n",
      "Epoch: 458 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.035 | Train Acc: 98.48%\n",
      "\t Val. Loss: 0.803 |  Val. Acc: 87.52%\n",
      "Epoch: 459 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.032 | Train Acc: 98.70%\n",
      "\t Val. Loss: 0.776 |  Val. Acc: 87.06%\n",
      "Epoch: 460 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.030 | Train Acc: 98.63%\n",
      "\t Val. Loss: 0.816 |  Val. Acc: 88.06%\n",
      "Epoch: 461 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.035 | Train Acc: 98.49%\n",
      "\t Val. Loss: 0.776 |  Val. Acc: 87.48%\n",
      "Epoch: 462 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.036 | Train Acc: 98.44%\n",
      "\t Val. Loss: 0.754 |  Val. Acc: 87.58%\n",
      "Epoch: 463 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.033 | Train Acc: 98.56%\n",
      "\t Val. Loss: 0.793 |  Val. Acc: 86.91%\n",
      "Epoch: 464 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.036 | Train Acc: 98.41%\n",
      "\t Val. Loss: 0.778 |  Val. Acc: 87.43%\n",
      "Epoch: 465 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.033 | Train Acc: 98.48%\n",
      "\t Val. Loss: 0.788 |  Val. Acc: 87.30%\n",
      "Epoch: 466 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.030 | Train Acc: 98.60%\n",
      "\t Val. Loss: 0.861 |  Val. Acc: 86.69%\n",
      "Epoch: 467 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.035 | Train Acc: 98.37%\n",
      "\t Val. Loss: 0.843 |  Val. Acc: 87.63%\n",
      "Epoch: 468 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.031 | Train Acc: 98.66%\n",
      "\t Val. Loss: 0.834 |  Val. Acc: 87.17%\n",
      "Epoch: 469 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.036 | Train Acc: 98.31%\n",
      "\t Val. Loss: 0.828 |  Val. Acc: 86.89%\n",
      "Epoch: 470 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.44%\n",
      "\t Val. Loss: 0.772 |  Val. Acc: 87.91%\n",
      "Epoch: 471 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.033 | Train Acc: 98.60%\n",
      "\t Val. Loss: 0.745 |  Val. Acc: 87.91%\n",
      "Epoch: 472 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.027 | Train Acc: 98.89%\n",
      "\t Val. Loss: 0.850 |  Val. Acc: 87.39%\n",
      "Epoch: 473 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.034 | Train Acc: 98.38%\n",
      "\t Val. Loss: 0.813 |  Val. Acc: 87.13%\n",
      "Epoch: 474 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.030 | Train Acc: 98.56%\n",
      "\t Val. Loss: 0.904 |  Val. Acc: 87.28%\n",
      "Epoch: 475 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.26%\n",
      "\t Val. Loss: 0.771 |  Val. Acc: 87.35%\n",
      "Epoch: 476 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.32%\n",
      "\t Val. Loss: 0.736 |  Val. Acc: 87.24%\n",
      "Epoch: 477 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.041 | Train Acc: 98.26%\n",
      "\t Val. Loss: 0.703 |  Val. Acc: 87.65%\n",
      "Epoch: 478 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.34%\n",
      "\t Val. Loss: 0.663 |  Val. Acc: 87.84%\n",
      "Epoch: 479 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.033 | Train Acc: 98.42%\n",
      "\t Val. Loss: 0.730 |  Val. Acc: 87.65%\n",
      "Epoch: 480 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.035 | Train Acc: 98.46%\n",
      "\t Val. Loss: 0.738 |  Val. Acc: 87.91%\n",
      "Epoch: 481 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.038 | Train Acc: 98.35%\n",
      "\t Val. Loss: 0.747 |  Val. Acc: 88.32%\n",
      "Epoch: 482 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.034 | Train Acc: 98.33%\n",
      "\t Val. Loss: 0.792 |  Val. Acc: 87.58%\n",
      "Epoch: 483 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.035 | Train Acc: 98.41%\n",
      "\t Val. Loss: 0.854 |  Val. Acc: 87.21%\n",
      "Epoch: 484 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.031 | Train Acc: 98.62%\n",
      "\t Val. Loss: 0.795 |  Val. Acc: 87.69%\n",
      "Epoch: 485 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.030 | Train Acc: 98.46%\n",
      "\t Val. Loss: 0.734 |  Val. Acc: 88.02%\n",
      "Epoch: 486 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.27%\n",
      "\t Val. Loss: 0.782 |  Val. Acc: 87.41%\n",
      "Epoch: 487 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.034 | Train Acc: 98.46%\n",
      "\t Val. Loss: 0.721 |  Val. Acc: 88.06%\n",
      "Epoch: 488 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.035 | Train Acc: 98.34%\n",
      "\t Val. Loss: 0.755 |  Val. Acc: 88.19%\n",
      "Epoch: 489 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.036 | Train Acc: 98.39%\n",
      "\t Val. Loss: 0.767 |  Val. Acc: 87.32%\n",
      "Epoch: 490 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.031 | Train Acc: 98.67%\n",
      "\t Val. Loss: 0.836 |  Val. Acc: 87.39%\n",
      "Epoch: 491 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.029 | Train Acc: 98.59%\n",
      "\t Val. Loss: 0.883 |  Val. Acc: 87.56%\n",
      "Epoch: 492 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.034 | Train Acc: 98.36%\n",
      "\t Val. Loss: 0.829 |  Val. Acc: 87.69%\n",
      "Epoch: 493 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.031 | Train Acc: 98.55%\n",
      "\t Val. Loss: 0.754 |  Val. Acc: 87.93%\n",
      "Epoch: 494 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.033 | Train Acc: 98.40%\n",
      "\t Val. Loss: 0.737 |  Val. Acc: 86.78%\n",
      "Epoch: 495 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.13%\n",
      "\t Val. Loss: 0.714 |  Val. Acc: 88.11%\n",
      "Epoch: 496 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.033 | Train Acc: 98.48%\n",
      "\t Val. Loss: 0.826 |  Val. Acc: 88.11%\n",
      "Epoch: 497 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.25%\n",
      "\t Val. Loss: 0.740 |  Val. Acc: 87.61%\n",
      "Epoch: 498 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.036 | Train Acc: 98.39%\n",
      "\t Val. Loss: 0.751 |  Val. Acc: 87.56%\n",
      "Epoch: 499 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.032 | Train Acc: 98.58%\n",
      "\t Val. Loss: 0.762 |  Val. Acc: 87.50%\n",
      "Epoch: 500 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.032 | Train Acc: 98.49%\n",
      "\t Val. Loss: 0.779 |  Val. Acc: 88.28%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 500\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "        \n",
    "    end_time = time.time()\n",
    "        \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), MODEL_OUTPUT_FILENAME)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    performance_data['Epoch'].append(epoch)\n",
    "    performance_data['Train Loss'].append(train_loss)\n",
    "    performance_data['Train Acc'].append(train_acc*100)\n",
    "    performance_data['Val. Loss'].append(valid_loss)\n",
    "    performance_data['Val. Acc'].append(valid_acc*100)\n",
    "    performance_data['Epoch Time elapsed'].append(end_time - start_time)\n",
    "    \n",
    "#torch.backends.cudnn.version()\n",
    "#torch.backends.cudnn.enabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train Loss</th>\n",
       "      <th>Train Acc</th>\n",
       "      <th>Val. Loss</th>\n",
       "      <th>Val. Acc</th>\n",
       "      <th>Epoch Time elapsed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.576698</td>\n",
       "      <td>70.721066</td>\n",
       "      <td>0.386551</td>\n",
       "      <td>82.462725</td>\n",
       "      <td>4.865226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.405422</td>\n",
       "      <td>82.076784</td>\n",
       "      <td>0.338668</td>\n",
       "      <td>85.654106</td>\n",
       "      <td>4.999137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.351576</td>\n",
       "      <td>84.598214</td>\n",
       "      <td>0.308255</td>\n",
       "      <td>87.194479</td>\n",
       "      <td>4.874531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.318866</td>\n",
       "      <td>86.433669</td>\n",
       "      <td>0.298047</td>\n",
       "      <td>87.628506</td>\n",
       "      <td>4.981327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.312140</td>\n",
       "      <td>86.948398</td>\n",
       "      <td>0.293364</td>\n",
       "      <td>88.128064</td>\n",
       "      <td>5.030830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0.033017</td>\n",
       "      <td>98.482044</td>\n",
       "      <td>0.825533</td>\n",
       "      <td>88.105086</td>\n",
       "      <td>5.395586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.037358</td>\n",
       "      <td>98.253385</td>\n",
       "      <td>0.740406</td>\n",
       "      <td>87.605528</td>\n",
       "      <td>5.383445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.036313</td>\n",
       "      <td>98.386769</td>\n",
       "      <td>0.751093</td>\n",
       "      <td>87.562126</td>\n",
       "      <td>5.327579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0.031771</td>\n",
       "      <td>98.577318</td>\n",
       "      <td>0.762476</td>\n",
       "      <td>87.496596</td>\n",
       "      <td>5.369121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.031680</td>\n",
       "      <td>98.494912</td>\n",
       "      <td>0.779307</td>\n",
       "      <td>88.279548</td>\n",
       "      <td>5.430913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Train Loss  Train Acc  Val. Loss   Val. Acc  Epoch Time elapsed\n",
       "Epoch                                                                 \n",
       "0        0.576698  70.721066   0.386551  82.462725            4.865226\n",
       "1        0.405422  82.076784   0.338668  85.654106            4.999137\n",
       "2        0.351576  84.598214   0.308255  87.194479            4.874531\n",
       "3        0.318866  86.433669   0.298047  87.628506            4.981327\n",
       "4        0.312140  86.948398   0.293364  88.128064            5.030830\n",
       "...           ...        ...        ...        ...                 ...\n",
       "495      0.033017  98.482044   0.825533  88.105086            5.395586\n",
       "496      0.037358  98.253385   0.740406  87.605528            5.383445\n",
       "497      0.036313  98.386769   0.751093  87.562126            5.327579\n",
       "498      0.031771  98.577318   0.762476  87.496596            5.369121\n",
       "499      0.031680  98.494912   0.779307  88.279548            5.430913\n",
       "\n",
       "[700 rows x 5 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "performance_df = pd.DataFrame.from_dict(performance_data)\n",
    "performance_df = performance_df.set_index('Epoch')\n",
    "performance_df.to_csv(path_or_buf=f'{MODEL_OUTPUT_FILENAME}_result.csv')\n",
    "performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "performance_df = pd.read_csv(f'{MODEL_OUTPUT_FILENAME}_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Train Loss</th>\n",
       "      <th>Train Acc</th>\n",
       "      <th>Val. Loss</th>\n",
       "      <th>Val. Acc</th>\n",
       "      <th>Epoch Time elapsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.576698</td>\n",
       "      <td>70.721066</td>\n",
       "      <td>0.386551</td>\n",
       "      <td>82.462725</td>\n",
       "      <td>4.865226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.405422</td>\n",
       "      <td>82.076784</td>\n",
       "      <td>0.338668</td>\n",
       "      <td>85.654106</td>\n",
       "      <td>4.999137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.351576</td>\n",
       "      <td>84.598214</td>\n",
       "      <td>0.308255</td>\n",
       "      <td>87.194479</td>\n",
       "      <td>4.874531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.318866</td>\n",
       "      <td>86.433669</td>\n",
       "      <td>0.298047</td>\n",
       "      <td>87.628506</td>\n",
       "      <td>4.981327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.312140</td>\n",
       "      <td>86.948398</td>\n",
       "      <td>0.293364</td>\n",
       "      <td>88.128064</td>\n",
       "      <td>5.030830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Epoch  Train Loss  Train Acc  Val. Loss   Val. Acc  Epoch Time elapsed\n",
       "0      0    0.576698  70.721066   0.386551  82.462725            4.865226\n",
       "1      1    0.405422  82.076784   0.338668  85.654106            4.999137\n",
       "2      2    0.351576  84.598214   0.308255  87.194479            4.874531\n",
       "3      3    0.318866  86.433669   0.298047  87.628506            4.981327\n",
       "4      4    0.312140  86.948398   0.293364  88.128064            5.030830"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df.set_index('Epoch',drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train Loss</th>\n",
       "      <th>Train Acc</th>\n",
       "      <th>Val. Loss</th>\n",
       "      <th>Val. Acc</th>\n",
       "      <th>Epoch Time elapsed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.576698</td>\n",
       "      <td>70.721066</td>\n",
       "      <td>0.386551</td>\n",
       "      <td>82.462725</td>\n",
       "      <td>4.865226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.405422</td>\n",
       "      <td>82.076784</td>\n",
       "      <td>0.338668</td>\n",
       "      <td>85.654106</td>\n",
       "      <td>4.999137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.351576</td>\n",
       "      <td>84.598214</td>\n",
       "      <td>0.308255</td>\n",
       "      <td>87.194479</td>\n",
       "      <td>4.874531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.318866</td>\n",
       "      <td>86.433669</td>\n",
       "      <td>0.298047</td>\n",
       "      <td>87.628506</td>\n",
       "      <td>4.981327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.312140</td>\n",
       "      <td>86.948398</td>\n",
       "      <td>0.293364</td>\n",
       "      <td>88.128064</td>\n",
       "      <td>5.030830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0.033017</td>\n",
       "      <td>98.482044</td>\n",
       "      <td>0.825533</td>\n",
       "      <td>88.105086</td>\n",
       "      <td>5.395586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.037358</td>\n",
       "      <td>98.253385</td>\n",
       "      <td>0.740406</td>\n",
       "      <td>87.605528</td>\n",
       "      <td>5.383445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.036313</td>\n",
       "      <td>98.386769</td>\n",
       "      <td>0.751093</td>\n",
       "      <td>87.562126</td>\n",
       "      <td>5.327579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0.031771</td>\n",
       "      <td>98.577318</td>\n",
       "      <td>0.762476</td>\n",
       "      <td>87.496596</td>\n",
       "      <td>5.369121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.031680</td>\n",
       "      <td>98.494912</td>\n",
       "      <td>0.779307</td>\n",
       "      <td>88.279548</td>\n",
       "      <td>5.430913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Train Loss  Train Acc  Val. Loss   Val. Acc  Epoch Time elapsed\n",
       "Epoch                                                                 \n",
       "0        0.576698  70.721066   0.386551  82.462725            4.865226\n",
       "1        0.405422  82.076784   0.338668  85.654106            4.999137\n",
       "2        0.351576  84.598214   0.308255  87.194479            4.874531\n",
       "3        0.318866  86.433669   0.298047  87.628506            4.981327\n",
       "4        0.312140  86.948398   0.293364  88.128064            5.030830\n",
       "...           ...        ...        ...        ...                 ...\n",
       "495      0.033017  98.482044   0.825533  88.105086            5.395586\n",
       "496      0.037358  98.253385   0.740406  87.605528            5.383445\n",
       "497      0.036313  98.386769   0.751093  87.562126            5.327579\n",
       "498      0.031771  98.577318   0.762476  87.496596            5.369121\n",
       "499      0.031680  98.494912   0.779307  88.279548            5.430913\n",
       "\n",
       "[500 rows x 5 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_df = performance_df[~performance_df.index.duplicated(keep='first')]\n",
    "performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train Loss</th>\n",
       "      <th>Train Acc</th>\n",
       "      <th>Val. Loss</th>\n",
       "      <th>Val. Acc</th>\n",
       "      <th>Epoch Time elapsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.057330</td>\n",
       "      <td>97.518085</td>\n",
       "      <td>0.681153</td>\n",
       "      <td>87.359120</td>\n",
       "      <td>5.403847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.051098</td>\n",
       "      <td>2.238968</td>\n",
       "      <td>0.116505</td>\n",
       "      <td>0.600540</td>\n",
       "      <td>0.103120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.027469</td>\n",
       "      <td>70.721066</td>\n",
       "      <td>0.293364</td>\n",
       "      <td>82.462725</td>\n",
       "      <td>4.865226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.036439</td>\n",
       "      <td>97.827744</td>\n",
       "      <td>0.648118</td>\n",
       "      <td>87.084482</td>\n",
       "      <td>5.367186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.040862</td>\n",
       "      <td>98.183600</td>\n",
       "      <td>0.716348</td>\n",
       "      <td>87.431917</td>\n",
       "      <td>5.411764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.049850</td>\n",
       "      <td>98.369323</td>\n",
       "      <td>0.758075</td>\n",
       "      <td>87.735737</td>\n",
       "      <td>5.453374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.576698</td>\n",
       "      <td>98.894817</td>\n",
       "      <td>0.904434</td>\n",
       "      <td>88.431032</td>\n",
       "      <td>5.654510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Train Loss   Train Acc   Val. Loss    Val. Acc  Epoch Time elapsed\n",
       "count  500.000000  500.000000  500.000000  500.000000          500.000000\n",
       "mean     0.057330   97.518085    0.681153   87.359120            5.403847\n",
       "std      0.051098    2.238968    0.116505    0.600540            0.103120\n",
       "min      0.027469   70.721066    0.293364   82.462725            4.865226\n",
       "25%      0.036439   97.827744    0.648118   87.084482            5.367186\n",
       "50%      0.040862   98.183600    0.716348   87.431917            5.411764\n",
       "75%      0.049850   98.369323    0.758075   87.735737            5.453374\n",
       "max      0.576698   98.894817    0.904434   88.431032            5.654510"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "performance_df[['Train Loss','Train Acc','Val. Loss','Val. Acc']].plot(figsize=(15,8)).figure.savefig('output/opinion_polarity/opinion_polarity_training.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load up the parameters that gave us the best validation loss and try these on the test set - which gives us our best results so far!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(MODEL_OUTPUT_FILENAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.288 | Test Acc: 88.82%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "We'll then use the model to test the sentiment of some sequences. We tokenize the input sequence, trim it down to the maximum length, add the special tokens to either side, convert it to a tensor, add a fake batch dimension and then pass it through our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(model, tokenizer, sentence):\n",
    "    model.eval()\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    indexed = [init_token_idx] + tokenizer.convert_tokens_to_ids(tokens) + [eos_token_idx]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004528311546891928"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, tokenizer, \"This film is terrible\") # ~ 0.02264 after IMDB training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4945968687534332"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, tokenizer, \"This film is great\") # ~ 0.94110 after IMDB training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3239787817001343"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, tokenizer, \"itâ€™s amazing how our city loves him\") # ~ 0.94110 after IMDB training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10930893570184708"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.bbc.com/news/world-us-canada-53293542\n",
    "\n",
    "predict_sentiment(model, tokenizer, '''\n",
    "US President Donald Trump has used an Independence Day address to vow to defeat the \"radical left\" as protests sweep the country.\n",
    "\n",
    "Striking a combative tone, Mr Trump said he would \"fight... to preserve American way of life\", while railing at \"mobs\" targeting historical monuments.\n",
    "\n",
    "Ahead of his speech, Black Lives Matter protesters gathered nearby.\n",
    "\n",
    "Mr Trump's 2020 election rival Joe Biden said everyone deserved \"a full share of the American dream\".\n",
    "\n",
    "The 4 July holiday marks the nation's declaration of independence from Britain in 1776 and is one of the most important days in the national calendar.\n",
    "\n",
    "Historically presidents have used the occasion to deliver speeches extolling the virtues of unity. Last year Mr Trump spoke of the \"extraordinary heritage\" of the country at an event with a militaristic theme that involved Air Force flyovers and tanks parked on display.\n",
    "\n",
    "This year, Mr Trump's address was again followed by a flyover involving various aircraft, including B-52 bombers and F-35 fighter jets.''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
